{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import PIL\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = tf.keras.applications.VGG19(include_top=True, weights=None)\n",
    "vgg.load_weights('vgg19_weights_tf_dim_ordering_tf_kernels.h5') # we have downloaded the imagenet weighgt instead of downloading it again and again\n",
    "for layers in vgg.layers:\n",
    "  print(f\"{layers.name} ---> {layers.output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "  gram_matrix = tf.expand_dims(result, axis=0)\n",
    "  input_shape = tf.shape(input_tensor)\n",
    "  i_j = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "  return gram_matrix/i_j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vgg():\n",
    "  vgg = tf.keras.applications.VGG19(include_top=True, weights=None)\n",
    "  vgg.load_weights('vgg19_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "  vgg.trainable = False\n",
    "  content_layers = ['block4_conv2']\n",
    "  style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "  content_output = vgg.get_layer(content_layers[0]).output \n",
    "  style_output = [vgg.get_layer(style_layer).output for style_layer in style_layers]\n",
    "  gram_style_output = [gram_matrix(output_) for output_ in style_output]\n",
    "\n",
    "  model = Model([vgg.input], [content_output, gram_style_output])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_object(style_outputs, content_outputs, style_target, content_target):\n",
    "  style_weight = 1e-2\n",
    "  content_weight = 1e-1\n",
    "  content_loss = tf.reduce_mean((content_outputs - content_target)**2)\n",
    "  style_loss = tf.add_n([tf.reduce_mean((output_ - target_)**2) for output_, target_ in zip(style_outputs, style_target)])\n",
    "  total_loss = content_weight*content_loss + style_weight*style_loss\n",
    "  return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(content_image_path,style_image_path):\n",
    "    content_image = cv2.resize(cv2.imread(content_image_path), (224, 224))\n",
    "    content_image = tf.image.convert_image_dtype(content_image, tf.float32)\n",
    "    style_image = cv2.resize(cv2.imread(style_image_path), (224, 224))\n",
    "    style_image = tf.image.convert_image_dtype(style_image, tf.float32)\n",
    "\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.imshow(cv2.cvtColor(np.array(content_image), cv2.COLOR_BGR2RGB))\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.imshow(cv2.cvtColor(np.array(style_image), cv2.COLOR_BGR2RGB))\n",
    "    # plt.show()\n",
    "\n",
    "    return content_image,style_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_path = 'content'\n",
    "style_path = 'styles'\n",
    "content_images_paths = []\n",
    "style_images_paths = []\n",
    "\n",
    "for path in os.listdir(content_path):\n",
    "    content_images_paths.append(os.path.join(content_path,path))\n",
    "    print(os.path.join(content_path,path))\n",
    "\n",
    "for path in os.listdir(style_path):\n",
    "    style_images_paths.append(os.path.join(style_path,path))\n",
    "    print(os.path.join(style_path,path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pathc in content_images_paths:\n",
    "    content_image = cv2.resize(cv2.imread(pathc), (224, 224))\n",
    "    content_image = tf.image.convert_image_dtype(content_image, tf.float32)\n",
    "    plt.plot()\n",
    "    plt.imshow(cv2.cvtColor(np.array(content_image), cv2.COLOR_BGR2RGB))\n",
    "    plt.title(pathc[8:-4])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paths in style_images_paths:\n",
    "    style_image = cv2.resize(cv2.imread(paths), (224, 224))\n",
    "    style_image = tf.image.convert_image_dtype(style_image, tf.float32)\n",
    "    plt.plot()\n",
    "    plt.imshow(cv2.cvtColor(np.array(style_image), cv2.COLOR_BGR2RGB))\n",
    "    plt.title(paths[7:-4])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(image, epoch,content_target,style_target,):\n",
    "  with tf.GradientTape() as tape:\n",
    "    output = vgg_model(image*255)\n",
    "    loss = loss_object(output[1], output[0], style_target, content_target)\n",
    "  gradient = tape.gradient(loss, image)\n",
    "  opt.apply_gradients([(gradient, image)])\n",
    "  image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0))\n",
    "\n",
    "  if epoch % 5 ==0:\n",
    "    tf.print(f\"Loss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pathc in content_images_paths:\n",
    "    for paths in style_images_paths:\n",
    "        print(pathc,paths)\n",
    "        c,s = load_image(pathc,paths)\n",
    "        opt = tf.optimizers.Adam(learning_rate=0.01, beta_1=0.99, epsilon=1e-1)\n",
    "        # print(c,s)\n",
    "        vgg_model = load_vgg()\n",
    "        content_target = vgg_model(np.array([c*255]))[0]\n",
    "        style_target = vgg_model(np.array([s*255]))[1]\n",
    "\n",
    "        EPOCHS = 20\n",
    "        image = tf.image.convert_image_dtype(c, tf.float32)\n",
    "        image = tf.Variable([image])\n",
    "        print(\"-------------------------------------------------------START TRAINING---------------------------------------------------------\")\n",
    "        for i in range(EPOCHS):\n",
    "            train_step(image, i,content_target,style_target)\n",
    "\n",
    "        print(\"-------------------------------------------------------END TRAINING-----------------------------------------------------------\")\n",
    "\n",
    "        print(f\"###########################---INPUT_IMAGE = {pathc[8:-4]} and STYLE_IMAGE = {paths[7:-4]}####################################\")\n",
    "        tensor = image*255\n",
    "        tensor = np.array(tensor, dtype=np.uint8)\n",
    "        if np.ndim(tensor)>3:\n",
    "            assert tensor.shape[0] == 1\n",
    "            tensor = tensor[0]\n",
    "        tensor =  PIL.Image.fromarray(tensor)\n",
    "        plt.imshow(cv2.cvtColor(np.array(tensor), cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        # break\n",
    "        print(\"##############################################################################################################################\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "style",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
